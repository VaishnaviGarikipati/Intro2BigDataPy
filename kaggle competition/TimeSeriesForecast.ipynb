{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Used https://machinelearningmastery.com/seed-state-lstms-time-series-forecasting-python/ to forecasting time series with lstm\n",
    "# Team name: BigData\n",
    "# Team Members Sudha Guda, Soujanya Dawalagiri, Vaishnavi Garikipati\n",
    "\n",
    "# %% [code]\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# %% [code]\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# %% [code]\n",
    "# df_wk1 = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/train.csv\")\n",
    "df_wk1 = pd.read_csv(\"/kaggle/input/countrydatafile/coordinates.csv\")\n",
    "train_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\n",
    "\n",
    "# %% [code]\n",
    "train_df[\"Date\"] = train_df[\"Date\"].astype(\"datetime64[ms]\")\n",
    "train_df[\"days\"] = (train_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n",
    "train_df[\"weekend\"] = train_df[\"Date\"].dt.dayofweek//5\n",
    "test_df[\"Date\"] = test_df[\"Date\"].astype(\"datetime64[ms]\")\n",
    "test_df[\"days\"] = (test_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n",
    "test_df[\"weekend\"] = test_df[\"Date\"].dt.dayofweek//5\n",
    "\n",
    "# %% [code]\n",
    "train_df['Province_State'].fillna(\"\", inplace=True)\n",
    "test_df['Province_State'].fillna(\"\", inplace=True)\n",
    "df_wk1['Province/State'].fillna(\"\", inplace=True)\n",
    "\n",
    "# %% [code]\n",
    "train_df['location'] = ['_'.join(x) for x in zip(train_df['Country_Region'], train_df['Province_State'])]\n",
    "test_df['location'] = ['_'.join(x) for x in zip(test_df['Country_Region'], test_df['Province_State'])]\n",
    "df_wk1['location'] = ['_'.join(x) for x in zip(df_wk1['Country/Region'], df_wk1['Province/State'])]\n",
    "\n",
    "# %% [code]\n",
    "# df_location_cord = df_wk1.groupby(\"location\")[[\"location\", \"Lat\", \"Long\"]]#.reset_index()\n",
    "df_location_cord = df_wk1.drop_duplicates('location')[['location', 'Lat', 'Long']]#.drop('ConfirmedCases', 'Fatalities', 'Id', 'Province/State', 'Country/Region', 'Date')\n",
    "\n",
    "# %% [code]\n",
    "# # sub = train_df.merge(pred_cases, how='left', on=['geo', 'day'])\n",
    "# df_location_cord.head(11)\n",
    "\n",
    "# %% [code]\n",
    "train_df_cord = pd.merge(train_df, df_location_cord, on='location', how='inner')\n",
    "test_df_cord = pd.merge(test_df, df_location_cord, on='location', how='inner')\n",
    "\n",
    "# %% [code]\n",
    "# loc_group = [\"Country_Region\", \"Province_State\"]\n",
    "\n",
    "# %% [code]\n",
    "# train_df.groupby('location')[\"ConfirmedCases\"].shift(1)#.tail(10)\n",
    "\n",
    "# %% [code]\n",
    "TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n",
    "features = [\"Lat\", \"Long\"]\n",
    "for s in range(1, 6):\n",
    "    for col in TARGETS:\n",
    "        train_df_cord[\"prev_{}_{}\".format(col, s)] = train_df_cord.groupby('location')[col].shift(s)\n",
    "#         test_df_cord[\"prev_{}_{}\".format(col, s)] = test_df_cord.groupby('location')[col].shift(s)\n",
    "        features.append(\"prev_{}_{}\".format(col, s))\n",
    "\n",
    "# %% [code]\n",
    "# df_location_cord.location.unique()\n",
    "# test_df.head(11)\n",
    "train_df_cord = train_df_cord[train_df_cord[\"Date\"] >= train_df_cord[\"Date\"].min() + timedelta(days=5)].copy()\n",
    "\n",
    "# %% [code]\n",
    "# dev_df.columns\n",
    "tst_start_dt = test_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n",
    "test_days = (train_df[\"Date\"].max() - tst_start_dt).days + 1\n",
    "\n",
    "dev_df, tst_df = train_df_cord[train_df_cord[\"Date\"] < tst_start_dt].copy(), train_df_cord[train_df_cord[\"Date\"] >= tst_start_dt].copy()\n",
    "\n",
    "# %% [code]\n",
    "# test_df = test_df.merge(tst_df[[\"Date\",\"location\"] + TARGETS], how=\"left\", on=[\"Date\", \"location\"])\n",
    "\n",
    "# %% [code]\n",
    "#     def nn_block(input_layer, size, dropout_rate, activation):\n",
    "#         out_layer = KL.Dense(size, activation=None)(input_layer)\n",
    "#         #out_layer = KL.BatchNormalization()(out_layer)\n",
    "#         out_layer = KL.Activation(activation)(out_layer)\n",
    "#         out_layer = KL.Dropout(dropout_rate)(out_layer)\n",
    "#         return out_layer\n",
    "\n",
    "\n",
    "#     def get_model():\n",
    "#         inp = KL.Input(shape=(len(features),))\n",
    "\n",
    "#         hidden_layer = nn_block(inp, 128, 0.0, \"relu\")\n",
    "#         hidden_layer = nn_block(hidden_layer, 64, 0.0, \"relu\")\n",
    "#         gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n",
    "#         hidden_layer = nn_block(hidden_layer, 48, 0.0, \"relu\")\n",
    "#         hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n",
    "#         hidden_layer = KL.multiply([hidden_layer, gate_layer])\n",
    "\n",
    "#         out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n",
    "\n",
    "#         model = tf.keras.models.Model(inputs=[inp], outputs=out)\n",
    "#         return model\n",
    "\n",
    "# %% [code]\n",
    "feat_confirm = ['Lat', 'Long', 'prev_ConfirmedCases_1', 'prev_ConfirmedCases_2', 'prev_ConfirmedCases_3', 'prev_ConfirmedCases_4', 'prev_ConfirmedCases_5']\n",
    "feat_fatal = ['Lat', 'Long', 'prev_Fatalities_1', 'prev_Fatalities_2', 'prev_Fatalities_3', 'prev_Fatalities_4', 'prev_Fatalities_5']\n",
    "model_confirm = Sequential()\n",
    "model_confirm.add(LSTM(50, activation='relu', input_shape=(len(feat_confirm), 1)))\n",
    "model_confirm.add(Dense(1))\n",
    "model_confirm.compile(optimizer='adam', loss='mean_squared_logarithmic_error')\n",
    "\n",
    "# %% [code]\n",
    "def conv_raw(df_raw, features):\n",
    "    df_tmp = df_raw[features]\n",
    "    df_tmp['single_input_vector'] = df_tmp.apply(tuple, axis=1).apply(list)\n",
    "    df_tmp_list = np.array(df_tmp['single_input_vector'].tolist())\n",
    "    X_df_tmp_list = df_tmp_list.reshape((df_tmp_list.shape[0], df_tmp_list.shape[1], 1))\n",
    "    return X_df_tmp_list\n",
    "\n",
    "# %% [code]\n",
    "# X_conf = dev_df[feat_confirm]\n",
    "y_conf = dev_df['ConfirmedCases']\n",
    "\n",
    "# %% [code]\n",
    "train_X_conf = conv_raw(dev_df, feat_confirm)\n",
    "\n",
    "# %% [code]\n",
    "# X_conf['single_input_vector'] = X_conf.apply(tuple, axis=1).apply(list)\n",
    "# list_X_conf = np.array(X_conf['single_input_vector'].tolist())\n",
    "\n",
    "# %% [code]\n",
    "# train_X_conf = list_X_conf.reshape((list_X_conf.shape[0], list_X_conf.shape[1], 1))\n",
    "\n",
    "# %% [code]\n",
    "# # Pad your sequences so they are the same length\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# max_sequence_length = X.cumulative_input_vectors.apply(len).max()\n",
    "# # Save it as a list   \n",
    "# padded_sequences = pad_sequences(X.cumulative_input_vectors.tolist(), max_sequence_length).tolist()\n",
    "# X['padded_input_vectors'] = pd.Series(padded_sequences).apply(np.asarray)\n",
    "\n",
    "# %% [code]\n",
    "# # Extract your training data\n",
    "# X_train_init = np.asarray(X.padded_input_vectors)\n",
    "# # Use hstack to and reshape to make the inputs a 3d vector\n",
    "# X_train = np.hstack(X_train_init).reshape(len(X),max_sequence_length,len(input_cols))\n",
    "# # y_train = np.hstack(np.asarray(df.output_vector)).reshape(len(df),len(output_cols))\n",
    "\n",
    "# %% [code]\n",
    "# # Extract your training data\n",
    "# X_train_init = np.asarray(X.cumulative_input_vectors)\n",
    "# # Use hstack to and reshape to make the inputs a 3d vector\n",
    "# X_train = np.hstack(X_train_init).reshape(len(X_train_init),len(X_train_init[0]),1)\n",
    "# # y_train = np.hstack(np.asarray(y.output_vector)).reshape(len(df),len(output_cols))\n",
    "\n",
    "# %% [code]\n",
    "# X_train_init.shape\n",
    "\n",
    "# %% [code]\n",
    "history_confirm = model_confirm.fit(train_X_conf, y_conf, epochs=10, batch_size=32, validation_split=0.1, verbose = 1)#,  shuffle=False)\n",
    "\n",
    "# %% [code]\n",
    "loss = history_confirm.history['loss']\n",
    "val_loss = history_confirm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# %% [code]\n",
    "tst_df_X_confirm = conv_raw(tst_df, feat_confirm)\n",
    "ypred_test_df_confirm = model_confirm.predict(tst_df_X_confirm, verbose=1)\n",
    "\n",
    "# %% [code]\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "X_fatal = dev_df[feat_fatal]\n",
    "y_fatal = dev_df['Fatalities']\n",
    "model_fatal = Sequential()\n",
    "model_fatal.add(LSTM(50, activation='relu', input_shape=(len(feat_fatal), 1)))\n",
    "model_fatal.add(Dense(1))\n",
    "model_fatal.compile(optimizer='adam', loss='mean_squared_logarithmic_error')\n",
    "\n",
    "# %% [code]\n",
    "# X_fatal['single_input_vector'] = X_fatal.apply(tuple, axis=1).apply(list)\n",
    "# list_X_fatal = np.array(X_fatal['single_input_vector'].tolist())\n",
    "# train_X_fatal = list_X_fatal.reshape((list_X_fatal.shape[0], list_X_fatal.shape[1], 1))\n",
    "train_X_fatal = conv_raw(X_fatal, feat_fatal)\n",
    "\n",
    "# %% [code]\n",
    "history_fatal = model_fatal.fit(train_X_fatal, y_fatal, epochs=10, batch_size=32, validation_split=0.1, verbose = 1)#,  shuffle=False)\n",
    "\n",
    "# %% [code]\n",
    "loss = history_fatal.history['loss']\n",
    "val_loss = history_fatal.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# %% [code]\n",
    "tst_df_X_fatal = conv_raw(tst_df, feat_fatal)\n",
    "ypred_test_df_fatal = model_fatal.predict(tst_df_X_fatal, verbose=1)\n",
    "\n",
    "# %% [code]\n",
    "# temp_df = test_df_private.loc[test_df_private[\"Date\"] == SUB_FIRST].copy()\n",
    "# # print(temp_df.columns)\n",
    "# temp_df_list_confirm = conv_raw(temp_df, feat_confirm)\n",
    "# temp_df_list_fatal = conv_raw(temp_df, feat_fatal)\n",
    "# y_pred_confirm = model_confirm.predict(temp_df_list_confirm, verbose=1)\n",
    "# y_pred_fatal = model_confirm.predict(temp_df_list_fatal, verbose=1)\n",
    "\n",
    "# %% [code]\n",
    "test_df_public = test_df[test_df[\"Date\"] <= train_df[\"Date\"].max()].copy()\n",
    "test_df_private = test_df[test_df[\"Date\"] > train_df[\"Date\"].max()].copy()\n",
    "\n",
    "pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n",
    "#sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n",
    "#                                    how=\"left\", on=[\"Date\"] + loc_group)\n",
    "test_df_public = test_df_public.merge(tst_df[[\"Date\",\"location\"] + TARGETS], how=\"left\", on=[\"Date\",\"location\"])\n",
    "\n",
    "SUB_FIRST = test_df_private[\"Date\"].min()\n",
    "SUB_DAYS = (test_df_private[\"Date\"].max() - test_df_private[\"Date\"].min()).days + 1\n",
    "\n",
    "test_df_private = train_df.append(test_df_private, sort=False)\n",
    "\n",
    "for s in range(1, 6):\n",
    "    for col in TARGETS:\n",
    "        test_df_private[\"prev_{}_{}\".format(col, s)] = test_df_private.groupby('location')[col].shift(s)\n",
    "\n",
    "test_df_private = test_df_private[test_df_private[\"Date\"] >= SUB_FIRST].copy()\n",
    "test_df_private = pd.merge(test_df_private, df_location_cord, on='location', how='inner')\n",
    "\n",
    "# %% [code]\n",
    "# test_df_private.loc[test_df_private[\"Date\"] == SUB_FIRST, \"pred_{}\".format('confirmed')] = y_pred_confirm\n",
    "\n",
    "# %% [code]\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# test_df_private.head(11)\n",
    "\n",
    "# %% [code]\n",
    "# # test_df_private.iloc[test_df_private.ForecastId==13419]\n",
    "# test_df_private.loc[test_df_private['ForecastId'] == 14.0]\n",
    "\n",
    "# %% [code]\n",
    "# test_df_public.tail(11)\n",
    "\n",
    "# %% [code]\n",
    "def predict_cases(org_df, first_date, num_days, model_val, stat, features):\n",
    "    temp_df = org_df.loc[org_df[\"Date\"] == first_date].copy()\n",
    "    # print(temp_df.columns)\n",
    "    temp_df_list = conv_raw(temp_df, features)\n",
    "    y_pred = model_val.predict(temp_df_list, verbose=1)\n",
    "    print(y_pred)\n",
    "    org_df.loc[org_df[\"Date\"] == SUB_FIRST, \"pred_{}\".format(stat)] = y_pred\n",
    "    y_prevs = [None]*5\n",
    "    for i in range(1, 5):\n",
    "        y_prevs[i] = temp_df[['prev_{}_{}'.format(stat, i)]].values\n",
    "    #     y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n",
    "    for d in range(1, num_days):\n",
    "        date_val = first_date + timedelta(days=d)\n",
    "    #     test_df_private[]\n",
    "        temp_df = org_df.loc[org_df[\"Date\"] == date_val].copy()\n",
    "        temp_df['prev_'+stat+'_1'] = y_pred\n",
    "#         print(temp_df[['prev_ConfirmedCases_1','prev_ConfirmedCases_2']].head(11))\n",
    "        for i in range(2, 6):\n",
    "            temp_df[['prev_{}_{}'.format(stat,i)]] = y_prevs[i-1]\n",
    "#         print(temp_df[['prev_ConfirmedCases_1','prev_ConfirmedCases_2']].head(11))\n",
    "        temp_df_list = conv_raw(temp_df, features)\n",
    "        y_pred = model_val.predict(temp_df_list, verbose=1)\n",
    "        y_prevs = [None, y_pred] + y_prevs[1:-1]\n",
    "        print(y_pred)\n",
    "        org_df.loc[org_df[\"Date\"] == date_val, \"pred_{}\".format(stat)] = y_pred\n",
    "    return org_df\n",
    "    #     break\n",
    "    # #     temp_df[prev_targets] = y_pred\n",
    "\n",
    "# %% [code]\n",
    "test_private_res_confirm = predict_cases(test_df_private, SUB_FIRST, SUB_DAYS, model_confirm, 'ConfirmedCases', feat_confirm)\n",
    "test_private_res_fatal = predict_cases(test_df_private, SUB_FIRST, SUB_DAYS, model_fatal, 'Fatalities', feat_fatal)\n",
    "\n",
    "# %% [code]\n",
    "new_test_df = test_private_res_fatal[['ForecastId','pred_ConfirmedCases', 'pred_Fatalities']]\n",
    "new_test_df = new_test_df.rename(columns={'pred_ConfirmedCases': 'ConfirmedCases', 'pred_Fatalities': 'Fatalities'})\n",
    "\n",
    "# %% [code]\n",
    "test_private_res_fatal.columns\n",
    "\n",
    "# %% [code]\n",
    "# test_private_res_confirm[['pred_ConfirmedCases']].head(100)\n",
    "# test_private_res_fatal.head(11)\n",
    "\n",
    "# %% [code]\n",
    "test_df_public[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n",
    "\n",
    "# %% [code]\n",
    "test_private_res_fatal[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n",
    "\n",
    "# %% [code]\n",
    "# final_dataset = test_df_public[['ForecastId', 'ConfirmedCases', 'Fatalities']].append(test_private_res_fatal[['ForecastId', 'ConfirmedCases', 'Fatalities']])\n",
    "final_dataset = pd.concat([test_df_public[['ForecastId', 'ConfirmedCases', 'Fatalities']],test_private_res_fatal[['ForecastId', 'ConfirmedCases', 'Fatalities']]], axis=0)\n",
    "\n",
    "# %% [code]\n",
    "final_dataset['ForecastId'] = final_dataset['ForecastId'].astype(int)\n",
    "final_dataset.fillna(0, inplace=True)\n",
    "final_dataset.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# # %% [code]\n",
    "# final_dataset.duplicated(subset=['ForecastId']).any()\n",
    "\n",
    "# # %% [code]\n",
    "# test_df_private.loc[test_df_private[\"Date\"] == SUB_FIRST, \"pred_{}\".format('confirmed')] = y_pred_confirm\n",
    "# y_prevs_confirm = [None]*5\n",
    "# for i in range(1, 5):\n",
    "#     y_prevs_confirm[i] = temp_df[['prev_ConfirmedCases_{}'.format(i)]].values\n",
    "# #     y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n",
    "# for d in range(1, SUB_DAYS):\n",
    "#     date_val = SUB_FIRST + timedelta(days=d)\n",
    "# #     test_df_private[]\n",
    "#     temp_df = test_df_private.loc[test_df_private[\"Date\"] == date_val].copy()\n",
    "#     temp_df['prev_ConfirmedCases_1'] = y_pred_confirm\n",
    "#     print(temp_df[['prev_ConfirmedCases_1','prev_ConfirmedCases_2']].head(11))\n",
    "#     for i in range(2, 6):\n",
    "#         temp_df[['prev_ConfirmedCases_{}'.format(i)]] = y_prevs_confirm[i-1]\n",
    "#     print(temp_df[['prev_ConfirmedCases_1','prev_ConfirmedCases_2']].head(11))\n",
    "#     temp_df_list = conv_raw(temp_df, feat_confirm)\n",
    "#     y_pred_confirm = model_confirm.predict(temp_df_list, verbose=1)\n",
    "#     y_prevs_confirm = [None, y_pred_confirm] + y_prevs_confirm[1:-1]\n",
    "#     test_df_private.loc[test_df_private[\"Date\"] == date_val, \"pred_{}\".format('confirmed')] = y_pred_confirm\n",
    "# #     break\n",
    "# # #     temp_df[prev_targets] = y_pred\n",
    "\n",
    "# # %% [code]\n",
    "# test_df_private.loc[test_df_private[\"Date\"] == SUB_FIRST, \"pred_{}\".format('fatal')] = y_pred_fatal\n",
    "# y_prevs_confirm = [None]*5\n",
    "# for i in range(1, 5):\n",
    "#     y_prevs_confirm[i] = temp_df[['prev_ConfirmedCases_{}'.format(i)]].values\n",
    "#     y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n",
    "# for d in range(1, SUB_DAYS):\n",
    "#     date_val = SUB_FIRST + timedelta(days=d)\n",
    "# #     test_df_private[]\n",
    "#     temp_df = test_df_private.loc[test_df_private[\"Date\"] == date_val].copy()\n",
    "#     temp_df['prev_ConfirmedCases_1'] = y_pred_confirm\n",
    "#     print(temp_df[['prev_ConfirmedCases_1','prev_ConfirmedCases_2']].head(11))\n",
    "#     for i in range(2, 6):\n",
    "#         temp_df[['prev_ConfirmedCases_{}'.format(i)]] = y_prevs_confirm[i-1]\n",
    "#     print(temp_df[['prev_ConfirmedCases_1','prev_ConfirmedCases_2']].head(11))\n",
    "#     temp_df_list = conv_raw(temp_df, feat_confirm)\n",
    "#     y_pred_confirm = model_confirm.predict(temp_df_list, verbose=1)\n",
    "#     y_prevs_confirm = [None, y_pred_confirm] + y_prevs_confirm[1:-1]\n",
    "#     test_df_private.loc[test_df_private[\"Date\"] == date_val, \"pred_{}\".format('confirmed')] = y_pred_confirm\n",
    "#     break\n",
    "# #     temp_df[prev_targets] = y_pred\n",
    "\n",
    "# %% [code]\n",
    "# temp_df = test_df_private.loc[test_df_private[\"Date\"] == date_val].copy()\n",
    "\n",
    "# %% [code]\n",
    "# temp_df['prev_ConfirmedCases_1'] = y_pred_confirm\n",
    "# print(temp_df[['prev_ConfirmedCases_1','prev_ConfirmedCases_2']].head(100))\n",
    "\n",
    "# %% [code]\n",
    "# for i in range(2, 6):\n",
    "#     temp_df[['prev_ConfirmedCases_{}'.format(i)]] = y_prevs[i-1]\n",
    "# print(temp_df[['prev_ConfirmedCases_1','prev_ConfirmedCases_2']].head(100))\n",
    "\n",
    "# %% [code]\n",
    "# y_prevs[1]\n",
    "\n",
    "# %% [code]\n",
    "# date_val = SUB_FIRST + timedelta(days=2)\n",
    "# temp_df1 = test_df_private.loc[test_df_private[\"Date\"] == date_val].copy()\n",
    "\n",
    "# %% [code]\n",
    "# temp_df1[['prev_ConfirmedCases_1','prev_Fatalities_1','prev_ConfirmedCases_2','prev_Fatalities_2']].head(50)\n",
    "\n",
    "# %% [code]\n",
    "# temp_df1.head(11)\n",
    "\n",
    "# %% [code]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
